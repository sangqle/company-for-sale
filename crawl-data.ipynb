{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/mydisk/.pyenv/versions/3.8.16/envs/crawl-data-for-sale/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.12.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting certifi>=2021.10.8\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting urllib3[socks]<3,>=1.26\n",
      "  Using cached urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.4-py3-none-any.whl (17 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sortedcontainers\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting idna\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting attrs>=20.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sortedcontainers, urllib3, sniffio, pysocks, idna, h11, exceptiongroup, certifi, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.1.0 certifi-2023.7.22 exceptiongroup-1.1.3 h11-0.14.0 idna-3.4 outcome-1.2.0 pysocks-1.7.1 selenium-4.12.0 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.22.2 trio-websocket-0.10.4 urllib3-2.0.4 wsproto-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/mnt/mydisk/.pyenv/versions/3.8.16/envs/crawl-data-for-sale/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using legacy 'setup.py install' for bs4, since package 'wheel' is not installed.\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "  Running setup.py install for bs4 ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.5\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/mnt/mydisk/.pyenv/versions/3.8.16/envs/crawl-data-for-sale/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def append_to_csv(file_path, data, header):\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    # Open the CSV file in append mode\n",
    "    with open(file_path, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "        # Create a CSV writer\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # If the file is new, write the header\n",
    "        if not file_exists:\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "        # Split the data string into individual values\n",
    "        values = data.split(', ')\n",
    "\n",
    "        # Write the values to the CSV file\n",
    "        csv_writer.writerow(values)\n",
    "\n",
    "# Example usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Function to scroll to the end of the page and click the \"Tiếp\" link\n",
    "def scroll_to_end_and_click_next(driver):\n",
    "    while True:\n",
    "        # Scroll to the end of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for a short time to load more content if necessary (adjust this as needed)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Find the \"Tiếp\" (Next) link\n",
    "        next_link = driver.find_element_by_link_text('Tiếp')\n",
    "\n",
    "        # If the \"Tiếp\" link is not displayed, break the loop (reached the last page)\n",
    "        if not next_link.is_displayed():\n",
    "            break\n",
    "\n",
    "        # Click the \"Tiếp\" link\n",
    "        next_link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extractInfo(soup):\n",
    "    try:\n",
    "        nganh_listing_txt = ''\n",
    "        logo_url = ''\n",
    "        company_name = ''\n",
    "        description = ''\n",
    "        address = ''\n",
    "        email = ''\n",
    "        website = ''\n",
    "        phone_numbers = []\n",
    "        district = ''\n",
    "        is_verified = 'yes'\n",
    "\n",
    "        try:\n",
    "            logo_url = soup.find('div', class_='logo_congty').find('img')['src']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Extract company name\n",
    "        try:\n",
    "            company_name_elem = soup.find('div', class_='listings_center')\n",
    "            if company_name_elem is not None:\n",
    "                company_name_elem = company_name_elem.find('a')\n",
    "                company_name = company_name_elem.text.strip() if company_name_elem else ''\n",
    "            else:\n",
    "                company_name_elem = soup.find('div', class_='listings_center_khongxacthuc')\n",
    "                company_name_elem = company_name_elem.find('a')\n",
    "                if company_name_elem:\n",
    "                    company_name = company_name_elem.text.strip() if company_name_elem else ''\n",
    "                    is_verified = 'no'\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Find the <div> with class \"logo_congty_diachi\"\n",
    "        try:\n",
    "            logo_congty_diachi_div = soup.find('div', class_=['logo_congty_diachi', 'listing_diachi_nologo'])\n",
    "\n",
    "            # Extract nganh_listing_txt\n",
    "            nganh_listing_txt_elem = logo_congty_diachi_div.find('span', class_='nganh_listing_txt')\n",
    "            nganh_listing_txt = nganh_listing_txt_elem.text.strip() if nganh_listing_txt_elem else ''\n",
    "\n",
    "            # Extract the address\n",
    "            address_elem = logo_congty_diachi_div.find('small')\n",
    "            address = address_elem.text.strip() if address_elem else ''\n",
    "            # Extract the phone numbers\n",
    "            phone_numbers = []\n",
    "            listing_dienthoai_div = logo_congty_diachi_div.find('div', class_='listing_dienthoai')\n",
    "            if listing_dienthoai_div:\n",
    "                phone_links = listing_dienthoai_div.find_all('a', href=True)\n",
    "                for phone_link in phone_links:\n",
    "                    hef = phone_link.get('href', '').strip()\n",
    "                    if hef.startswith('tel:'):\n",
    "                        phone_numbers.append(hef.replace('tel:', ''))\n",
    "            else:\n",
    "                phone_link = logo_congty_diachi_div.find('div', class_='p-2 pt-0 ps-0 pe-4 pb-0').find('a')\n",
    "                hef = phone_link.get('href', '').strip()\n",
    "                if hef.startswith('tel:'):\n",
    "                    phone_numbers.append(hef.replace('tel:', ''))\n",
    "        \n",
    "            # Define a regular expression pattern to match \"Q. [District]\" before \"Tp. Hồ Chí Minh\"\n",
    "            pattern = r'(.+?),\\s*Tp\\. Hồ Chí Minh'\n",
    "            # Use regular expression to find the matching part in the address\n",
    "            match = re.search(pattern, address)\n",
    "            # Extract the matching part if found\n",
    "            if match:\n",
    "                extracted_text = match.group(1).strip()\n",
    "                district = extracted_text.split(', ')[-1]\n",
    "            else:\n",
    "                extracted_text = None\n",
    "        except:\n",
    "            pass\n",
    "        # Extract description\n",
    "        try:\n",
    "            description_elem = soup.find('div', class_='div_textqc').find('small', class_='text_qc')\n",
    "            description = description_elem.text.strip() if description_elem else ''\n",
    "        except:\n",
    "            pass\n",
    "        # Extract email and website\n",
    "        try:\n",
    "            email_web_div = soup.find('div', class_='email_web_section')\n",
    "            if email_web_div:\n",
    "                # Find the <a> elements within the div\n",
    "                email_web_links = email_web_div.find_all('a')\n",
    "                for link in email_web_links:\n",
    "                    href = link.get('href', '').strip()\n",
    "                    if href.startswith('mailto:'):\n",
    "                        email = href.replace('mailto:', '')\n",
    "                    elif href and not href.startswith('javascript:'):\n",
    "                        website = href\n",
    "        except:\n",
    "            pass\n",
    "        return [nganh_listing_txt, logo_url, company_name, description, address, district, email, website, is_verified, ', '.join(phone_numbers)]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'output.csv'  # Replace with your desired CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Đại Tân á - Chi Nhánh Công Ty TNHH Thương Mại & Dịch Vụ Đại Tân á', '', 'Phòng 11, Tòa Nhà Nam Việt, 107 Bến Vân Đồn, P. 9, Q. 4, Tp. Hồ Chí Minh, Việt Nam', 'Q. 4', 'data-saigon@vnn.vn', '', 'no', '02839432512']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Cổ Phần Vận Tải Xuất Nhập Khẩu Hàng Cảng Thành', '', 'Lầu 4 , Tòa Nhà U-Silk, Số 7 Lam Sơn, P. 2, Q. Tân Bình, Tp. Hồ Chí Minh, Việt Nam', 'Q. Tân Bình', 'trung-nvocc@tropolis.com.vn', 'http://www.tropolis.com.sg', 'no', '0285472300']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Vận  Tải Trọng Hiếu', '', '4A/276 La Xuân Oai, Tăng Nhơn Phú A, Q. 9, Tp. Hồ Chí Minh, Việt Nam', 'Q. 9', 'trucking@thtshipping.com', 'http://www.thtshipping.com', 'no', '02837309742']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Vận Tải Và DVHH Nhật Bình', '', '15/4 Đường 1, KP. 6, Trường Thọ, Q. Thủ Đức, Tp. Hồ Chí Minh, Việt Nam', 'Q. Thủ Đức', 'truonggiabinh@vnn.vn', 'http://www.nhatbinhtrans.com', 'no', '02837310688']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Thương Mại Dịch Vụ Giao Nhận Vận Tải Phúc Hưng', '', '58/97 Phan Chu Trinh, P. 24, Q. Bình Thạnh, Tp. Hồ Chí Minh, Việt Nam', 'Q. Bình Thạnh', 'hoang.liet@vapulogistic.com', '', 'no', '02835515624']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Dịch Vụ & Vận Tải Toàn Cầu Ngôi Sao Việt Nam', '', '662B Bùi Đình Túy, P. 12, Q. Bình Thạnh, Tp. Hồ Chí Minh, Việt Nam', 'Q. Bình Thạnh', 'info@vinastarglobal.com', 'http://vinastarglobal.com', 'no', '02835117173']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Long Quy Logistics', '', '267/9B, Đường Số 11, Kp. Thái Bình, P. Long Bình, Q. 9, Tp. Hồ Chí Minh, Việt Nam', 'Q. 9', 'info@longquy.com', 'http://lql.vn', 'no', '02837251953']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Everstar Sealandair', '', '220/9 Nguyễn Trọng Tuyển, P. 8, Q. Phú Nhuận, Tp. Hồ Chí Minh, Việt Nam', 'Q. Phú Nhuận', 'head-bdt@everstarsealandair.com', 'http://everstarsealandair.com', 'no', '02838442967']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH MTV Vận Tải Khang Thịnh', '', '20 Đường 34, P. 12, Q. Gò Vấp, Tp. Hồ Chí Minh, Việt Nam', 'Q. Gò Vấp', 'info@khangthinhship.com.vn', 'http://khangthinhship.com.vn', 'no', '02839471059']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Giao Nhận Quang Khoa', '', 'C12/11, Quốc lộ 1A, ấp 3, X. Tân Kiên, H. Bình Chánh, Tp. Hồ Chí Minh, Việt Nam', 'H. Bình Chánh', 'info@qkl.vn', 'http://qkl.com.vn', 'no', '0913745454']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Tiếp Vận Quốc Tế Hải Nam', '', 'Lầu 4, 135A Pasteur, Phường 06, Quận 3, Tp. Hồ Chí Minh, Việt Nam', 'Quận 3', 'sales1@hainamship.com', 'http://hainamship.com', 'no', '0288272688']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Cổ Phần TM & DV Phi Thiên', '', '21 Phạm Cụ Lượng, P. 2, Q. Tân Bình, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Giao Nhận Vận Tải Quốc Tế Việt Long', '', '251/8 Lê Quang Định, P. 7, Q. Bình Thạnh, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Thương Mại Dịch Vụ Tiếp Vận VN', '', '283 Điện Biên Phủ, P.15, Q. Bình Thạnh, Tp. Hồ Chí Minh, Việt Nam', '', '', '', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Vận Tải World Blis', '', '5F, 307/6 Nguyễn Văn Trỗi, Q. Tân Bình, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Cổ Phần Quốc Tế Tất Thành Công', '', 'Số 248/1 Thái Thịnh Bình, Tp. Đà Nẵng, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Thương Mại Dịch Vụ Vận Tải Triệu Hưng', '', '2/23 Phan Thúc Duyện, P4, Q. Tân Bình Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Trung Mỹ - Công Ty TNHH MTV Vận Chuyển Thương Mại Trung Mỹ', '', '277/2-3 Phan Văn Khỏe, P. 5, Q. 6, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Thương Mại Chuyển Phát Quốc Tế', '', 'Tầng 5, Tòa Nhà SV Technologies, 2A Phan Thúc Duyện, P. 4, Q. Tân Bình, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Tiếp Vận Vũ Trụ', '', 'Lầu 4, 28E Nguyễn Hữu Cảnh, P. 22, Q. Bình Thạnh, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Một Thành Viên Thương Mại Dịch Vụ Vận Tải Việt Cam', '', 'A28 Hẻm 92, Đ. Phan Huy ích, P. 15, Q. Tân Bình, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Người Chinh Phục Thái Bình DươngPACIFIC CONQUEROR CO., LTD (Int’l Transport & Logistics)', '', 'Số 1 Đường Số 8, KP. 2, P. Linh Đông, Q. Thủ Đức, Tp. Hồ Chí Minh (TPHCM) , Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Lâm Gia Thịnh - Công Ty TNHH Lâm Gia Thịnh', '', '15 Đường Số 10, KDC Trung Sơn, H. Bình Chánh, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Chuyển Phát Nhanh CityLink Express Vietnam', '', '47 Trần Quốc Hoàn, P. 4, Q. Tân Bình, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Logistics Việt Nam', '', 'Số 75, Đường 2B, P. Bình Hưng Hoà B, Q. Bình Tân, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty Cổ Phần Giao Nhận Bến Thành', '', 'Tầng 4, Tòa Nhà Artex Saigon (Asb), 236-238 Nguyễn Công Trứ, P. Nguyễn Thái Bình, Q. 1, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Lê Hòa - Công Ty TNHH Thương Mại & Dịch Vụ Giao Nhận Lê Hòa', '', '42 Đại Lộ Võ Văn Kiệt, P. Nguyễn Thái Bình, Q. 1, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH International Trade Supplier Business', '', '49 Xóm Đất, P. 8, Q. 11, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Giao Nhận Vận Tải Phúc Đại Phát', '', '161 Âu Dương Lân, P. 2. Q. 8, Tp. Hồ Chí Minh, Việt Nam', '', '', '', 'yes', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Trans Global Solutions Việt Nam', '', 'E1/2H12 Quách Điêu, xã Vĩnh Lộc A, Bình Chánh Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Twin Logistics Việt Nam', '', '607-609, Tầng 3, Nguyễn Kiệm, P. 9, Q. Phú Nhuận, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH TM DV Vận Tải Huy Hà', '', '64/2B Bình Lợi, P. 13, Q. Bình Thạnh, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Sunshine Vina Logix', '', 'Số 03 Phan Văn Đạt, Phường Bến Nghé, Quận 1, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH TM DV HBT Logistics', '', 'Tòa nhà Wilton, 71/03 Nguyễn Văn Thương, P. 25, Bình Thạnh, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Win & Win Global Logistics Việt Nam', '', '43 Nguyễn Văn Mại, P. 4, Q. Tân Bình, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Tiếp Vận Tân Liêu', '', '131B Nguyễn Thị Minh Khai,            Phường Bến Thành, Quận 1, TP HCM Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Vận Tải Sao Đại Dương', '', '38-40 Nguyễn Đình Chính, P. 15, Q. Phú Nhuận, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n",
      "['công ty logistics  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Giao Nhận Đại Dương Toàn Cầu', '', '134/1 Cách Mạng Tháng Tám, P. 10, Q. 3, Tp. Hồ Chí Minh, Việt Nam', '', '', '#', 'no', '']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "excel_file_path = 'output.xlsx'\n",
    "\n",
    "# Function to extract page IDs and click each page to crawl data\n",
    "def crawl_multiple_pages(url, delay=20):\n",
    "    options = Options()\n",
    "    options.page_load_strategy = 'normal'\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(delay)\n",
    "\n",
    "    try:\n",
    "        # Extract the content inside the iframe\n",
    "        iframe_content = driver.page_source\n",
    "\n",
    "        # Parse the iframe content with BeautifulSoup\n",
    "        soup = BeautifulSoup(iframe_content, 'html.parser')\n",
    "\n",
    "        # Extract and process data as needed\n",
    "        div_list_company = soup.find_all('div', class_='div_list_cty')\n",
    "        div_list_company_element = div_list_company[0].find_all('div', class_='w-100 h-auto shadow rounded-3 bg-white p-2 mb-3')\n",
    "        print(len(div_list_company_element))\n",
    "        list_data = [] \n",
    "        for div_company in div_list_company_element:\n",
    "            data = extractInfo(div_company)\n",
    "            if data:\n",
    "                append_to_excel(data, 'logistic.xlsx')\n",
    "                print(data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "crawl_multiple_pages('https://trangvangvietnam.com/cateprovinces/484645/c%C3%B4ng-ty-logistics-%E1%BB%9F-t%E1%BA%A1i-tp.-h%E1%BB%93-ch%C3%AD-minh-(tphcm).html?page=8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "def append_to_excel(data, excel_file):\n",
    "    try:\n",
    "        # Load the existing workbook\n",
    "        workbook = openpyxl.load_workbook(excel_file)\n",
    "    except FileNotFoundError:\n",
    "        # If the file doesn't exist, create a new workbook\n",
    "        workbook = openpyxl.Workbook()\n",
    "\n",
    "    # Select the active sheet (usually the first sheet)\n",
    "    sheet = workbook.active\n",
    "\n",
    "    # Find the next available row in the sheet\n",
    "    next_row = sheet.max_row + 1\n",
    "\n",
    "    # Iterate through the data and write to the next available row\n",
    "    for col, value in enumerate(data, start=1):\n",
    "        cell = sheet.cell(row=next_row, column=col)\n",
    "        cell.value = value\n",
    "\n",
    "    # Save the workbook to the Excel file, overwriting the existing file\n",
    "    workbook.save(excel_file)\n",
    "\n",
    "# # Example usage:\n",
    "# data = [\n",
    "#     'bàn ghế nội thất  ở tại  tp. hồ chí minh', '', 'Công Ty TNHH Thiết Kế Thi Công Nội Thất Tinhome', '',\n",
    "#     'Số 6, Đường Số 8, P. Bình Hưng Hòa B, Q. Bình Tân, Tp. Hồ Chí Minh, Việt Nam', 'Q. Bình Tân',\n",
    "#     'tinhomevn@gmail.com', 'http://tinhome.vn', 'no', '0908391297'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify the Excel file path\n",
    "\n",
    "# Specify the Excel sheet name (optional)\n",
    "# sheet_name = 'Sheet1'\n",
    "\n",
    "# Call the function to write data to the Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9563.96s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/mnt/mydisk/.pyenv/versions/3.8.16/envs/crawl-data-for-sale/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl-data-for-sale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
